{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the training, test and store data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(\"Loading the training, test and store data\")\n",
    "types = {'CompetitionOpenSinceYear': np.dtype(int),'CompetitionOpenSinceMonth': np.dtype(int),'StateHoliday': np.dtype(str),'Promo2SinceWeek': np.dtype(int),'SchoolHoliday': np.dtype(float),'PromoInterval': np.dtype(str)}\n",
    "train = pd.read_csv(\"inputs/train.csv\", parse_dates=[2], dtype=types)\n",
    "test = pd.read_csv(\"inputs/test.csv\", parse_dates=[2], dtype=types)\n",
    "store = pd.read_csv(\"inputs/store.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data points considered outliers-\n",
      "Number of outliers in sales = 15863\n",
      "Number of outliers in customers = 22600\n",
      "Number of common outliers = 11334\n",
      "Dropping common outliers...\n",
      "1.90838602133 %  of data dropped\n"
     ]
    }
   ],
   "source": [
    "Q1 = np.percentile(train['Sales'], 25)\n",
    "Q3 = np.percentile(train['Sales'], 75)\n",
    "step = step = 1.5 * (Q3 - Q1)\n",
    "print \"Data points considered outliers-\"\n",
    "#display(train[~((train['Sales'] >= Q1 - step) & (train['Sales'] <= Q3 + step))])\n",
    "i=train[~((train['Sales'] >= Q1 - step) & (train['Sales'] <= Q3 + step))].index\n",
    "print \"Number of outliers in sales =\", len(i)\n",
    "Q1 = np.percentile(train['Customers'], 25)\n",
    "Q3 = np.percentile(train['Customers'], 75)\n",
    "step = step = 1.5 * (Q3 - Q1)\n",
    "#display(train[~((train['Customers'] >= Q1 - step) & (train['Customers'] <= Q3 + step))])\n",
    "j=train[~((train['Customers'] >= Q1 - step) & (train['Customers'] <= Q3 + step))].index\n",
    "print \"Number of outliers in customers =\", len(j)\n",
    "common_outliers = set(i).intersection(j)\n",
    "print \"Number of common outliers =\", len(common_outliers)\n",
    "print \"Dropping common outliers...\"\n",
    "train = train.drop(train.index[list(common_outliers)]).reset_index(drop = True)\n",
    "print float(len(common_outliers))*100/float(train.shape[0]), \"%  of data dropped\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(features, data):\n",
    "    # remove NaNs-\n",
    "    data.fillna(0, inplace=True)\n",
    "    #take the store as open if no value given-\n",
    "    data.loc[data.Open.isnull(), 'Open'] = 1\n",
    "    #only process stores which are open, otherwise whats the point:\n",
    "    data = data[data[\"Open\"] != 0]         \n",
    "    # Use some properties directly\n",
    "    features.extend(['Store', 'CompetitionDistance', 'Promo', 'Promo2', 'SchoolHoliday'])\n",
    "    # Label encode some features\n",
    "    features.extend(['StoreType', 'Assortment', 'StateHoliday'])\n",
    "    mappings = {'0':0, 'a':1, 'b':2, 'c':3, 'd':4}\n",
    "    data.StoreType.replace(mappings, inplace=True)\n",
    "    data.Assortment.replace(mappings, inplace=True)\n",
    "    data.StateHoliday.replace(mappings, inplace=True)\n",
    "\n",
    "    features.extend(['DayOfWeek', 'Month', 'Day', 'Year', 'WeekOfYear'])\n",
    "    data['Year'] = data.Date.dt.year\n",
    "    data['Month'] = data.Date.dt.month\n",
    "    data['Day'] = data.Date.dt.day\n",
    "    data['DayOfWeek'] = data.Date.dt.dayofweek\n",
    "    data['WeekOfYear'] = data.Date.dt.weekofyear\n",
    "  \n",
    "\n",
    "    # Calculate time competition open time in months\n",
    "    features.append('CompetitionOpen')\n",
    "    data['CompetitionOpen'] = 12 * (data.Year - data.CompetitionOpenSinceYear) + (data.Month - data.CompetitionOpenSinceMonth)\n",
    "    # Promo open time in months\n",
    "    features.append('PromoOpen')\n",
    "    data['PromoOpen'] = 12 * (data.Year - data.Promo2SinceYear) + (data.WeekOfYear - data.Promo2SinceWeek) / 4.0\n",
    "    data['PromoOpen'] = data.PromoOpen.apply(lambda x: x if x > 0 else 0)\n",
    "    data.loc[data.Promo2SinceYear == 0, 'PromoOpen'] = 0\n",
    "\n",
    "    # Indicate that sales on that day are in promo interval\n",
    "    features.append('IsPromoMonth')\n",
    "    month2str = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', 7:'Jul', 8:'Aug', 9:'Sept', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
    "    data['monthStr'] = data.Month.map(month2str)\n",
    "    data.loc[data.PromoInterval == 0, 'PromoInterval'] = ''\n",
    "    data['IsPromoMonth'] = 0\n",
    "    for interval in data.PromoInterval.unique():\n",
    "        if interval != '':\n",
    "            for month in interval.split(','):\n",
    "                data.loc[(data.monthStr == month) & (data.PromoInterval == interval), 'IsPromoMonth'] = 1\n",
    "    return features, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action = \"ignore\")\n",
    "train = pd.merge(train, store, on='Store')\n",
    "test = pd.merge(test, store, on='Store')\n",
    "features = []\n",
    "features, train= preprocess(features, train)\n",
    "\n",
    "#Drop rows where sales are zero\n",
    "train = train[train[\"Sales\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 368055 samples.\n",
      "Testing set has 122686 samples.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "# TODO: Shuffle and split the dataset into the number of training and testing points above\n",
    "X_train, X_test = cross_validation.train_test_split(train, test_size=0.25)\n",
    "\n",
    "# Show the results of the split\n",
    "print \"Training set has {} samples.\".format(X_train.shape[0])\n",
    "print \"Testing set has {} samples.\".format(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from time import time\n",
    "def train_regressor(rgr, X_train, y_train):\n",
    "    ''' Fits a regressor to the training data. '''\n",
    "    \n",
    "    # Start the clock, train the regressor, then stop the clock\n",
    "    start = time()\n",
    "    rgr.fit(X_train, y_train)\n",
    "    end = time()\n",
    "    \n",
    "    # Print the results\n",
    "    print \"Trained model in {:.4f} seconds\".format(end - start)\n",
    "\n",
    "    \n",
    "def predict_labels(rgr, features, target):\n",
    "    ''' Makes predictions using a fit regressor based on mean_squared_error '''\n",
    "    \n",
    "    # Start the clock, make predictions, then stop the clock\n",
    "    start = time()\n",
    "    y_pred = rgr.predict(features)\n",
    "    end = time()\n",
    "    \n",
    "    # Print and return results\n",
    "    print \"Made predictions in {:.4f} seconds.\".format(end - start)\n",
    "    #return mean_squared_error(target.values, y_pred)\n",
    "    return np.sqrt(((y_pred - target.values) ** 2).mean())\n",
    "\n",
    "\n",
    "\n",
    "def train_predict(rgr, X_train, y_train, X_test, y_test):\n",
    "    ''' Train and predict using a regressor based on F1 score. '''\n",
    "    \n",
    "    # Indicate the regressor and the training set size\n",
    "    print \"Training a {} using a training set size of {}. . .\".format(rgr.__class__.__name__, len(X_train))\n",
    "    \n",
    "    # Train the regressor\n",
    "    train_regressor(rgr, X_train, y_train)\n",
    "    \n",
    "    # Print the results of prediction for both training and testing\n",
    "    print \"mean_squared_error for training set: {:.4f}.\".format(predict_labels(rgr, X_train, y_train))\n",
    "    print \"mean_squared_error for test set: {:.4f}.\".format(predict_labels(rgr, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "rgr_A = DecisionTreeRegressor(random_state=0)\n",
    "rgr_B = KNeighborsRegressor()\n",
    "rgr_C = GradientBoostingRegressor()\n",
    "i=np.log1p(X_train.Sales)\n",
    "j=np.log1p(X_test.Sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a DecisionTreeRegressor using a training set size of 368055. . .\n",
      "Trained model in 5.7585 seconds\n",
      "Made predictions in 0.4843 seconds.\n",
      "mean_squared_error for training set: 0.0000.\n",
      "Made predictions in 0.1327 seconds.\n",
      "mean_squared_error for test set: 0.1859.\n"
     ]
    }
   ],
   "source": [
    "train_predict(rgr_A, X_train[features], i, X_test[features], j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a KNeighborsRegressor using a training set size of 368055. . .\n",
      "Trained model in 2.8538 seconds\n",
      "Made predictions in 21.9409 seconds.\n",
      "mean_squared_error for training set: 0.1954.\n",
      "Made predictions in 7.0320 seconds.\n",
      "mean_squared_error for test set: 0.2475.\n"
     ]
    }
   ],
   "source": [
    "train_predict(rgr_B, X_train[features], i, X_test[features], j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a GradientBoostingRegressor using a training set size of 368055. . .\n",
      "Trained model in 60.6633 seconds\n",
      "Made predictions in 0.9889 seconds.\n",
      "mean_squared_error for training set: 0.3150.\n",
      "Made predictions in 0.3285 seconds.\n",
      "mean_squared_error for test set: 0.3159.\n"
     ]
    }
   ],
   "source": [
    "train_predict(rgr_C, X_train[features], i, X_test[features], j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions in 0.0641 seconds.\n",
      "The best parameters values are: {'min_samples_split': 20, 'max_depth': None, 'min_samples_leaf': 8}\n",
      "RMSE value after grid search = 0.163918324729\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "i=i[X_train[features].index] #to avoid indexing error\n",
    "\n",
    "param_grid = {\"min_samples_split\": [20, 15, 30],\n",
    "              \"max_depth\": [None, 2, 5, 10],\n",
    "              \"min_samples_leaf\": [12, 8, 10]\n",
    "              }\n",
    "grid_search = GridSearchCV(rgr_A,param_grid=param_grid)\n",
    "grid_search.fit(X_train[features],i)\n",
    "x=predict_labels(grid_search, X_test[features], j)\n",
    "print \"The best parameters values are:\", grid_search.best_params_\n",
    "print \"RMSE value after grid search =\",x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 1 :DayOfWeek (0.254320)\n",
      "2. feature 0 :Store (0.206382)\n",
      "3. feature 2 :Date (0.172261)\n",
      "4. feature 13 :CompetitionOpenSinceYear (0.091545)\n",
      "5. feature 8 :SchoolHoliday (0.071241)\n",
      "6. feature 10 :Assortment (0.042359)\n",
      "7. feature 5 :Open (0.035158)\n",
      "8. feature 12 :CompetitionOpenSinceMonth (0.034278)\n",
      "9. feature 14 :Promo2 (0.033655)\n",
      "10. feature 6 :Promo (0.022376)\n",
      "11. feature 3 :Sales (0.011258)\n",
      "12. feature 9 :StoreType (0.011243)\n",
      "13. feature 11 :CompetitionDistance (0.005961)\n",
      "14. feature 4 :Customers (0.004231)\n",
      "15. feature 15 :Promo2SinceWeek (0.002166)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEKCAYAAAD+XoUoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFM1JREFUeJzt3X/wXXV95/HnK6RogWKrIl+BkrT+oJWutW43zS51/bLs\naqB1Q6ezbdAVS1eXzhZla7dF7bQJnWm3zo5u7bDWTYuMP9imA5bCrL9iV7+zsl0kWPy1JBBFQgIY\nRJBfahvCe/84J/Hy9Zt87zf53u9N7uf5mLmTe875fM7n87nJvO65n3NOTqoKSVIblo27A5KkpWPo\nS1JDDH1JaoihL0kNMfQlqSGGviQ1xNBXc5L8aZLfGXc/pHGI1+lrWEnuAp4DPAEEKOCFVfW1w9jn\ny4EPVtUPL0onjzJJrgJ2VtXvjbsvasPycXdAR5UCfq6qPrWI+9z35XFolZNjqmrvIvZnySTxl7aW\nnP/otFCZc2WyOsn/SfJQklv7I/h9234lyW1JHkny5ST/vl9/HPAR4JQkj/bbp5JcleT3B+q/PMnO\ngeWvJvntJJ8HHkuyLMlzk1yb5P4kX0nyxgMOYGD/+/ad5LeS7E5yT5K1Sc5NcnuSB5K8daDu+iTX\nJNnU9/eWJC8e2P5jST7Vfw5fTPKqWe2+O8mHkzwK/DvgNcBv9/u6vi93Wf85PZLkS0nOH9jH65J8\nOsl/SfJgP9Y1A9t/KMl7+3F8I8lfDWz7+f7v5qEkNyb5RwPbLkuyq29za5KzD/T56ShXVb58DfUC\nvgr8iznWnwI8ALyyXz6nX35Wv3wusLJ//zLgceAl/fLLgbtn7e8q4PcHlp9Spu/H3/XtPo3ui+gW\n4HeAY4CVwJeBf3WAcezff7/vPQN1Xw/cD3wQOA54EfAtYEVffj3w98Av9OV/E7izf78c2A5c1r8/\nG3gEeMFAuw8Bq/vlp80ea7/+F4GT+/f/BnhsYPl1ffu/2o/714B7Bup+GPgL4MS+Ty/r1/8UsBv4\n6b7ea/vP8fuAFwJ3D7RxOvAj4/735ms0L4/0tVB/3R9hPjhwFPlvgQ9X1ccBqup/0YXwef3yR6vq\nrv79p4HNdOF/ON5VVfdW1d8D/wR4dlX9QVXt7dv6c2DdkPv6B+APq5sm2gQ8G/jjqvpWVd0G3Ab8\n5ED5z1bVdX35d9KF9+r+dXxVvb2qnqhuGux/AhcM1L2+qm4C6Pv+ParqQ1W1u39/Dd0XyaqBIjuq\n6r1VVcD7gOcmeU6SKeCVwMVV9Uj/WXy6r/MG4D1VdUt1PkD35bEa2AscC/xEkuVVdXdVfXXIz05H\nGef0tVBr63vn9FcAvzQwlRG6f1ufBEhyLvB7dEeUy4DvB75wmP3YNav9U5M8OND+MuB/D7mvb/QB\nCvDt/s/7B7Z/GzhhYHn/VFNVVZJ76H51ZHBbbwdw6lx1DyTJhcBv0P1iATie7oton/0nzqvq20no\n+/cs4MGqemSO3a4ALhyY9grdUf4pVfXpJP8R2AC8KMnHgd+sqvvm66uOPoa+FmquOf2dwPur6uLv\nKZwcC1xL92vg+qp6Msl1A/uZ6yTu43RTK/s8d44yg/V2AndW1RlD9H8x7L/SKF3ingbcSzem02eV\nPR24fWB59nifspzkdGAjcHZV/d9+3a0c4FzKLDuBZyY5cY7g3wn8QVX957kqVtUmYFOSE/r2/4hu\nKkkTxukdLYYPAq9K8or+pOrT+xOkp9BNGxwLPNAH/rnAKwbq7gaeleTEgXWfA87rT0pOAZfO0/7N\nwKP9yd2nJzkmyZlJfnrxhvgU/zjJ+UmOoTsi/w5wE/AZ4PG+H8uTTAM/TzfHfiC7gR8dWD4eeBJ4\noP8sLwJ+YphOVXfp7EeBdyf5wb4P+6bR/gz4tSSrAJIcn+S8/s8XJjm7/4L+B7pfNk8O9UnoqGPo\nayHmvLSyqnYBa4G3AV+nm9L4T8CyqnoMeBNwTT/9sg64fqDu7XSheGd/nmAK+ADd9M9dwMfo5tkP\n2I+qepIuXF9Cd3LyfrqQO5FDc9Cj8b7/v0x3UvY1wC/08+d7gFfRnct4ALgCeG1VbT/AfgCuBM7c\nd46kqrbSnSe4iW4a50zgxgX097V091Fso/tCuRSgqj5LN69/Rf/3cAffPZJ/Gt2R/dfpfrGcBLwV\nTaShbs7qLwn7Y7oviSur6u2ztr+a7ooFgEeB/1BVX+i33QU8THfksKeqBk9ISUeVJOuB51XVhePu\ni3Qo5p3TT3cDyRV0l+HdC2xJcn1VbRsodifwz6vq4f4LYiPdVQHQhf10VT20uF2XJC3UMNM7q4Dt\nVbWj//m6ie6n/H5VdVNVPdwv3sRTr1bYdyWFJGnMhrl651SeepnZLp56zfBsr6c7mbRPAZ9IshfY\nWFV/tuBeSkeIqrp83H2QDseiXrLZ37p9EfCzA6vPqqr7kpxEF/5bq2q+E1OSpBEYJvTv4anXHp/W\nr3uK/v8f2QisGZy/33eDR1V9vb8+exVzXI2QxP/uU5IWqKqGuYdjv2Hm2rcAz0+yor+Odx1ww2CB\n/oaSD9FdnvaVgfXH9Td7kOR4uuuzv3SQzi/5a/369WP/vzAcs2N2zI75UF6HYt4j/aram+QSuv8v\nZd8lm1uTXNxtro3A7wLPpLspJHz30syTgev6o/jlwNVVtfmQeipJOmxDzelX1ceAM2at++8D799A\nd+PH7HpfpbthRpJ0BGj+Usrp6elxd2HJOeY2OGbN5Yh5XGKSOlL6IklHgyTUCE7kSpImhKEvSQ0x\n9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENf\nkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWp\nIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihPyZTUytJMtLX1NTKcQ9T0hFmqNBPsibJtiR3\nJLlsju2vTvL5/nVjkhcPW7dVu3fvAGqkr64NSfquVNXBCyTLgDuAc4B7gS3AuqraNlBmNbC1qh5O\nsgbYUFWrh6k7sI+ary+TJAldOI+0FVr6TKXWJKGqspA6wxzprwK2V9WOqtoDbALWDhaoqpuq6uF+\n8Sbg1GHrSpKWzjChfyqwc2B5F98N9bm8HvjoIdaVJI3Q8sXcWZKzgYuAnz2U+hs2bNj/fnp6munp\n6UXplyRNgpmZGWZmZg5rH8PM6a+mm6Nf0y+/Baiqevusci8GPgSsqaqvLKRuv805/cVvxTl9aYKN\nak5/C/D8JCuSHAusA26Y1fDpdIH/2n2BP2xdSdLSmXd6p6r2JrkE2Ez3JXFlVW1NcnG3uTYCvws8\nE3h3ukPYPVW16kB1RzYaSdJBzTu9s1Sc3hlJK07vSBNsVNM7kqQJYehLUkMMfUlqiKEvSQ0x9CWp\nIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi\n6EtSQwx9SWqIoS9JDTH0GzQ1tZIkI3tNTa0c9xAlHUCqatx9ACBJHSl9WQpJgFGPN8z1mY6+7bnb\nlbS4klBVWUgdj/QlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQ\nl6SGGPqS1BBDX5IaMlToJ1mTZFuSO5JcNsf2M5L8bZLvJHnzrG13Jfl8kluT3LxYHZckLdzy+Qok\nWQZcAZwD3AtsSXJ9VW0bKPYN4I3A+XPs4klguqoeWoT+SpIOwzBH+quA7VW1o6r2AJuAtYMFquqB\nqvos8MQc9TNkO5KkERsmjE8Fdg4s7+rXDauATyTZkuQNC+mcJGlxzTu9swjOqqr7kpxEF/5bq+rG\nJWhXkjTLMKF/D3D6wPJp/bqhVNV9/Z9fT3Id3XTRnKG/YcOG/e+np6eZnp4ethlJmngzMzPMzMwc\n1j7mfUZukmOA2+lO5N4H3AxcUFVb5yi7Hnisqt7RLx8HLKuqx5IcD2wGLq+qzXPU9Rm5i9+Kz8iV\nJtihPCN33iP9qtqb5BK6wF4GXFlVW5Nc3G2ujUlOBm4BfgB4MsmlwIuAk4DrklTf1tVzBb4kaWnM\ne6S/VDzSH0krHulLE+xQjvS9lFKSGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENf\nkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWp\nIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi\n6EtSQwx9SWqIoS9JDRkq9JOsSbItyR1JLptj+xlJ/jbJd5K8eSF1JUlLJ1V18ALJMuAO4BzgXmAL\nsK6qtg2UeTawAjgfeKiq3jls3YF91Hx9mSRJgFGPN8z1mY6+7bnblbS4klBVWUidYY70VwHbq2pH\nVe0BNgFrBwtU1QNV9VngiYXWlSQtnWFC/1Rg58Dyrn7dMA6nriRpkXkiV5IasnyIMvcApw8sn9av\nG8aC6m7YsGH/++npaaanp4dsRpIm38zMDDMzM4e1j2FO5B4D3E53MvY+4GbggqraOkfZ9cBjVfWO\nQ6jridzFb8UTudIEO5QTufMe6VfV3iSXAJvppoOurKqtSS7uNtfGJCcDtwA/ADyZ5FLgRVX12Fx1\nFzguSdIimfdIf6l4pD+SVjzSlybYqC7ZlCRNCENfkhpi6EtSQwx9LZmpqZUkGelramrluIcpHdE8\nkTsmLZ7IHeeYpUnkiVxJ0kEZ+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS\n1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0FcTRv3ULp/YpaOFT84aE5+cNWlt+8QuLT2f\nnCVJOihDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1Jaoih\nL0kNGSr0k6xJsi3JHUkuO0CZP0myPcnnkvzUwPq7knw+ya1Jbl6sjkuSFm75fAWSLAOuAM4B7gW2\nJLm+qrYNlDkXeF5VvSDJzwB/CqzuNz8JTFfVQ4vee0nSggxzpL8K2F5VO6pqD7AJWDurzFrg/QBV\n9RngGUlO7rdlyHYkSSM2TBifCuwcWN7VrztYmXsGyhTwiSRbkrzhUDsqSTp8807vLIKzquq+JCfR\nhf/WqrpxroIbNmzY/356eprp6ekl6J4kHR1mZmaYmZk5rH3M+7jEJKuBDVW1pl9+C1BV9faBMu8B\nPlVVf9kvbwNeXlW7Z+1rPfBoVb1zjnZ8XOLit3JEPTqwxTFLozSqxyVuAZ6fZEWSY4F1wA2zytwA\nXNh3YjXwzaraneS4JCf0648HXgF8aSEdlCQtnnmnd6pqb5JLgM10XxJXVtXWJBd3m2tjVX0kyXlJ\nvgw8DlzUVz8ZuC5J9W1dXVWbRzMUSdJ85p3eWSpO74yklSNqqqPFMUujNKrpHUnShDD0Jakhhr40\nQlNTK0ky0tfU1MpxD1NHEef0x6TF+W3HvLRta/I5py9JOihDX5IaYuhLUkMMfUlqiKEvTahRXznk\nVUNHJ6/eGROvZJm0th3zPlNTK9m9e8fIWj355BV87Wt3jWz/R5NDuXrH0B+TFsPAMU9a20dWu9De\nF46hfxQxDCatbcc87nbH3fY4eJ2+JOmgDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi\n6EvSYTqa/p8j78gdE+/UnLS2HfO42x1n2+Ns1ztyJUkHZOhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE\n0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSFDhX6SNUm2JbkjyWUHKPMnSbYn+VySlyyk\nriRpacwb+kmWAVcArwTOBC5I8mOzypwLPK+qXgBcDLxn2LrjNjMzM+4uSNKSGeZIfxWwvap2VNUe\nYBOwdlaZtcD7AarqM8Azkpw8ZN2xMvQltWSY0D8V2DmwvKtfN0yZYepKkpbIqE7kLuhJLpKkpbF8\niDL3AKcPLJ/Wr5td5ofnKHPsEHX36x45tvQuv/zysbS7FN+NB/5MR9v2uNodZ9uO+Uhod5xtj3PM\nwxsm9LcAz0+yArgPWAdcMKvMDcCvA3+ZZDXwzaraneSBIeoCLPg5j5KkhZs39Ktqb5JLgM1000FX\nVtXWJBd3m2tjVX0kyXlJvgw8Dlx0sLojG40k6aByoKfKS5ImT7N35LZ201iS05J8Msn/S/LFJG8a\nd5+WSpJlSf4uyQ3j7stSSPKMJNck2dr/ff/MuPs0akl+I8mXknwhydVJjh13nxZbkiuT7E7yhYF1\nP5Rkc5Lbk3w8yTPm20+ToX803DQ2Ak8Ab66qM4F/Cvx6A2Pe51LgtnF3Ygm9C/hIVf048JPARE+p\nJjkFeCPw0qp6Md209brx9mokrqLLrEFvAf6mqs4APgm8db6dNBn6HAU3jS22qvpaVX2uf/8YXRBM\n/D0TSU4DzgP+fNx9WQpJTgReVlVXAVTVE1X1yJi7tRSOAY5Pshw4Drh3zP1ZdFV1I/DQrNVrgff1\n798HnD/ffloN/aZvGkuyEngJ8Jnx9mRJ/Ffgt4BWTl79CPBAkqv6Ka2NSb5/3J0apaq6F3gHcDfd\nJeHfrKq/GW+vlsxzqmo3dAd2wHPmq9Bq6DcryQnAtcCl/RH/xEryc8Du/hdOaOOmweXAS4H/VlUv\nBb5FNwUwsZL8IN0R7wrgFOCEJK8eb6/GZt6Dm1ZDf5gbziZO/9P3WuADVXX9uPuzBM4C/nWSO4G/\nAM5O8v4x92nUdgE7q+qWfvlaui+BSfYvgTur6sGq2gv8FfDPxtynpbK7/3/OSDIF3D9fhVZDf/8N\nZ/1Z/nV0N5hNuvcCt1XVu8bdkaVQVW+rqtOr6kfp/o4/WVUXjrtfo9T/1N+Z5IX9qnOY/JPYdwOr\nkzw93W2r5zC5J69n/2K9AfiV/v3rgHkP5oa5I3fitHjTWJKzgNcAX0xyK93PwLdV1cfG2zONwJuA\nq5N8H3An/c2Sk6qqbk5yLXArsKf/c+N4e7X4kvwPYBp4VpK7gfXAHwHXJPlVYAfwS/Pux5uzJKkd\nrU7vSFKTDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhry/wGoaugu2DzKBwAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4e48b4dad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "importances = rgr_A.feature_importances_ \n",
    "# return the indices that would sort the importance, decreasing\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(15):\n",
    "    print(\"%d. feature %d :%s (%f)\" % (f + 1, indices[f],X_train.columns[indices[f]], importances[indices[f]]))\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(1,10), importances[indices[range(1,10)]]) \n",
    "plt.xlim([-1, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "p=[]\n",
    "p, test= preprocess(p, test)\n",
    "i=np.log1p(train.Sales)\n",
    "i=i[train[features].index] #to avoid indexing error\n",
    "rgr_A = DecisionTreeRegressor(min_samples_split= 20, max_depth= None, min_samples_leaf= 8)\n",
    "rgr_A.fit(train[features], i)\n",
    "#train over complete dataset and predict test-\n",
    "pred =rgr_A.predict(test[features])\n",
    "p_Sales =rgr_A.predict(test[features])\n",
    "test['p_Sales'] = np.expm1(p_Sales)\n",
    "test.to_csv(\"prediction.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
